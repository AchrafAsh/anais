{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import string\n",
    "import sys\n",
    "import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data import get_train_test_split, get_sentences\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "On concatene à notre vecteur (qui represente une suite de caractère) un vecteur qui représente la position des caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (Transformers)\n",
    "\n",
    "Définition du Transformers au niveau des caractères.\n",
    "Le modèle comprend:\n",
    "- Position Encoder - pour créer le vecteur de représentation des caractères\n",
    "- Encoder (Embeddings) - pour créer le vecteur de représentation de la chaine de caractères\n",
    "- Encoder Layers - Transformers (plusieurs couches)\n",
    "- Decoder - retourne un vecteur de représentation de la chaine de caractères prédite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANAIS(nn.Module):\n",
    "    \"\"\"Transformers based model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, num_heads, hidden_size, num_layers, dropout=0.5):\n",
    "        super(ANAIS, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(emb_dim, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(emb_dim, num_heads, hidden_size, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.encoder = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.decoder = nn.Linear(emb_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.emb_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "    def train(self, dataset, num_epochs=1, lr=5.0):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "        \n",
    "        model.train() # Turn on the train mode\n",
    "        total_loss = 0.\n",
    "        start_time = time.time()\n",
    "        src_mask = self.generate_square_subsequent_mask(bptt).to(device)\n",
    "        \n",
    "        for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "            optimizer.zero_grad()\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            \n",
    "            output = self.forward(data, src_mask)\n",
    "            loss = criterion(output.view(-1, ntokens), targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            log_interval = 200\n",
    "            \n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                      'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                      'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                        elapsed * 1000 / log_interval,\n",
    "                        cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "                \n",
    "    def evaluate(eval_model, data_source):\n",
    "        self.eval() # Turn on the evaluation mode\n",
    "        total_loss = 0.\n",
    "        src_mask = self.generate_square_subsequent_mask(bptt).to(device)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, data_source.size(0) - 1, bptt):\n",
    "                data, targets = get_batch(data_source, i)\n",
    "                if data.size(0) != bptt:\n",
    "                    src_mask = self.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "                output = self.forward(data, src_mask)\n",
    "                output_flat = output.view(-1, ntokens)\n",
    "                total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "        return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Custom Dataset and Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AISDataset(Dataset):\n",
    "    def __init__(self,filename,vocab):\n",
    "        df = pd.read_csv(filename)\n",
    "        df = df[['input','target','code']]\n",
    "        df = df[df['target'].notna()]\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.input = df['input']\n",
    "        self.target = df['target']\n",
    "        self.code = df['code']\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.target[idx], self.code[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a vocabulary out of words in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        word = str(word).lower()\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.idx2word[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "    def build_vocab(self, words):\n",
    "        counter = Counter()\n",
    "        for i, word in enumerate(words):\n",
    "            token = str(word).lower().strip()\n",
    "            if(token != ''):counter.update([token])\n",
    "\n",
    "        # Create a vocab wrapper and add some special tokens.\n",
    "        # self.add_word('<pad>')\n",
    "        # self.add_word('<start>')\n",
    "        # self.add_word('<end>')\n",
    "        self.add_word('<unk>')\n",
    "\n",
    "        # Add words to the vocabulary.\n",
    "        for word, count in counter.items():\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df[['input','target','code']]\n",
    "    df = df[df['target'].notna()]\n",
    "    \n",
    "    words = df['input'].unique().tolist() + df['target'].unique().tolist()\n",
    "    vocab = Vocab()\n",
    "    vocab.build_vocab(words)\n",
    "    \n",
    "    \n",
    "    data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "      return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "    \n",
    "    \n",
    "    dataset = AISDataset(filename, vocab)\n",
    "    dataloader = DataLoader(dataset)\n",
    "    \n",
    "    return vocab, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __main__\n",
    "vocab, dataloader = data_processing('./clean_dataset.csv')\n",
    "len(vocab)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = get_train_test_split('./clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab.stoi)\n",
    "emsize = 200 \n",
    "nhid = 200 \n",
    "nlayers = 2\n",
    "nhead = 2\n",
    "dropout = 0.2\n",
    "model = TransformerModel(ntokens, emsize,\n",
    "                         nhead, nhid,\n",
    "                         nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
